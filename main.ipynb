{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRINH THE SON - 20127617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIANTS object defines data configurations for different file sizes.\n",
    "# Each key represents the data size (e.g., \"100k\" for 100 thousand).\n",
    "VARIANTS = {\n",
    "    \"100k\": {\"filename\": \"u.data\", \"sep\": \"\\t\"},\n",
    "    \"1m\": {\"filename\": \"ratings.dat\", \"sep\": r\"::\"},\n",
    "    \"20m\": {\"filename\": \"ratings.csv\", \"sep\": \",\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chosen data variant (e.g., \"100k\", \"1m\", or \"20m\")\n",
    "variant = \"100k\"\n",
    "\n",
    "# Check if the chosen variant is a valid key in the VARIANTS object\n",
    "if variant not in VARIANTS:\n",
    "    # If not valid, raise an error\n",
    "    raise ValueError(\n",
    "        f\"Invalid variant: {variant}. Valid options are {list(VARIANTS.keys())}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the URL for downloading the data based on the chosen variant\n",
    "url = f\"http://files.grouplens.org/datasets/movielens/ml-{variant}.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from the VARIANTS object for the chosen variant\n",
    "variant_info = VARIANTS[variant]\n",
    "\n",
    "# Destructure filename property from the variant information\n",
    "filename = variant_info[\"filename\"]\n",
    "\n",
    "# Construct the directory name based on the variant\n",
    "dirname = f\"ml-{variant}\"\n",
    "\n",
    "# Construct the path to the downloaded zip file\n",
    "zip_path = os.path.join(dirname + \".zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen(url) as r, open(zip_path, \"wb\") as f:\n",
    "    shutil.copyfileobj(r, f)\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "    zf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zip file after extraction (optional)\n",
    "os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(dirname, filename)\n",
    "names = [\"u_id\", \"i_id\", \"rating\", \"timestamp\"]\n",
    "dtype = {\"u_id\": np.uint32, \"i_id\": np.uint32, \"rating\": np.float64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    csv_path,\n",
    "    names=names,\n",
    "    dtype=dtype,\n",
    "    header=0,\n",
    "    sep=VARIANTS[variant][\"sep\"],\n",
    ")\n",
    "\n",
    "df.sort_values(by=\"u_id\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sample(frac=0.8, random_state=7)\n",
    "val = df.drop(train.index.tolist()).sample(frac=0.5, random_state=8)\n",
    "test = df.drop(train.index.tolist()).drop(val.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialization(n_users, n_items, n_factors):\n",
    "    bu = np.zeros(n_users)\n",
    "    bi = np.zeros(n_items)\n",
    "\n",
    "    pu = np.random.normal(0, 0.1, (n_users, n_factors))\n",
    "    qi = np.random.normal(0, 0.1, (n_items, n_factors))\n",
    "\n",
    "    return bu, bi, pu, qi\n",
    "\n",
    "def _run_epoch(X, bu, bi, pu, qi, global_mean, n_factors, lr, reg):\n",
    "    for i in range(X.shape[0]):\n",
    "        user, item, rating = int(X[i, 0]), int(X[i, 1]), X[i, 2]\n",
    "\n",
    "        # Predict current rating\n",
    "        pred = global_mean + bu[user] + bi[item]\n",
    "\n",
    "        for factor in range(n_factors):\n",
    "            pred += pu[user, factor] * qi[item, factor]\n",
    "\n",
    "        err = rating - pred\n",
    "\n",
    "        # Update biases\n",
    "        bu[user] += lr * (err - reg * bu[user])\n",
    "        bi[item] += lr * (err - reg * bi[item])\n",
    "\n",
    "        # Update latent factors\n",
    "        for factor in range(n_factors):\n",
    "            puf = pu[user, factor]\n",
    "            qif = qi[item, factor]\n",
    "\n",
    "            pu[user, factor] += lr * (err * qif - reg * puf)\n",
    "            qi[item, factor] += lr * (err * puf - reg * qif)\n",
    "\n",
    "    return bu, bi, pu, qi\n",
    "\n",
    "def _compute_val_metrics(X_val, bu, bi, pu, qi, global_mean, n_factors):\n",
    "    residuals = []\n",
    "\n",
    "    for i in range(X_val.shape[0]):\n",
    "        user, item, rating = int(X_val[i, 0]), int(X_val[i, 1]), X_val[i, 2]\n",
    "        pred = global_mean\n",
    "\n",
    "        if user > -1:\n",
    "            pred += bu[user]\n",
    "\n",
    "        if item > -1:\n",
    "            pred += bi[item]\n",
    "\n",
    "        if (user > -1) and (item > -1):\n",
    "            for factor in range(n_factors):\n",
    "                pred += pu[user, factor] * qi[item, factor]\n",
    "\n",
    "        residuals.append(rating - pred)\n",
    "\n",
    "    residuals = np.array(residuals)\n",
    "    loss = np.square(residuals).mean()\n",
    "    rmse = np.sqrt(loss)\n",
    "    mae = np.absolute(residuals).mean()\n",
    "\n",
    "    return loss, rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from functools import wraps\n",
    "from math import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version():\n",
    "    return __version__\n",
    "\n",
    "\n",
    "def _timer(text=\"\"):\n",
    "    def decorator(func):\n",
    "\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end = time.time()\n",
    "\n",
    "            hours = trunc((end - start) / 3600)\n",
    "            minutes = trunc((end - start) / 60)\n",
    "            seconds = round((end - start) % 60)\n",
    "\n",
    "            if hours > 1:\n",
    "                print(\n",
    "                    text + \"{} hours {} min and {} sec\".format(hours, minutes, seconds)\n",
    "                )\n",
    "            elif hours == 1:\n",
    "                print(\n",
    "                    text + \"{} hour {} min and {} sec\".format(hours, minutes, seconds)\n",
    "                )\n",
    "            elif minutes >= 1:\n",
    "                print(text + \"{} min and {} sec\".format(minutes, seconds))\n",
    "            else:\n",
    "                print(text + \"{} sec\".format(seconds))\n",
    "\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=0.005,\n",
    "        reg=0.02,\n",
    "        n_epochs=20,\n",
    "        n_factors=100,\n",
    "        early_stopping=False,\n",
    "        shuffle=False,\n",
    "        min_delta=0.001,\n",
    "        min_rating=1,\n",
    "        max_rating=5,\n",
    "    ):\n",
    "\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_factors = n_factors\n",
    "        self.early_stopping = early_stopping\n",
    "        self.shuffle = shuffle\n",
    "        self.min_delta = min_delta\n",
    "        self.min_rating = min_rating\n",
    "        self.max_rating = max_rating\n",
    "\n",
    "    def fit(self, X, X_val=None):\n",
    "        X = self._preprocess_data(X)\n",
    "\n",
    "        if X_val is not None:\n",
    "            X_val = self._preprocess_data(X_val, train=False, verbose=False)\n",
    "            self._init_metrics()\n",
    "\n",
    "        self.global_mean_ = np.mean(X[:, 2])\n",
    "        self._run_sgd(X, X_val)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _preprocess_data(self, X, train=True, verbose=True):\n",
    "        print(\"Preprocessing data...\\n\")\n",
    "        X = X.copy()\n",
    "\n",
    "        if train:  # Mappings have to be created\n",
    "            user_ids = X[\"u_id\"].unique().tolist()\n",
    "            item_ids = X[\"i_id\"].unique().tolist()\n",
    "\n",
    "            n_users = len(user_ids)\n",
    "            n_items = len(item_ids)\n",
    "\n",
    "            user_idx = range(n_users)\n",
    "            item_idx = range(n_items)\n",
    "\n",
    "            self.user_mapping_ = dict(zip(user_ids, user_idx))\n",
    "            self.item_mapping_ = dict(zip(item_ids, item_idx))\n",
    "\n",
    "        X[\"u_id\"] = X[\"u_id\"].map(self.user_mapping_)\n",
    "        X[\"i_id\"] = X[\"i_id\"].map(self.item_mapping_)\n",
    "\n",
    "        # Tag validation set unknown users/items with -1 (enables\n",
    "        # `fast_methods._compute_val_metrics` detecting them)\n",
    "        X.fillna(-1, inplace=True)\n",
    "\n",
    "        X[\"u_id\"] = X[\"u_id\"].astype(np.int32)\n",
    "        X[\"i_id\"] = X[\"i_id\"].astype(np.int32)\n",
    "\n",
    "        return X[[\"u_id\", \"i_id\", \"rating\"]].values\n",
    "\n",
    "    def _init_metrics(self):\n",
    "        metrics = np.zeros((self.n_epochs, 3), dtype=float)\n",
    "        self.metrics_ = pd.DataFrame(metrics, columns=[\"Loss\", \"RMSE\", \"MAE\"])\n",
    "\n",
    "    def _run_sgd(self, X, X_val):\n",
    "        n_users = len(np.unique(X[:, 0]))\n",
    "        n_items = len(np.unique(X[:, 1]))\n",
    "\n",
    "        bu, bi, pu, qi = _initialization(n_users, n_items, self.n_factors)\n",
    "\n",
    "        # Run SGD\n",
    "        for epoch_ix in range(self.n_epochs):\n",
    "            start = self._on_epoch_begin(epoch_ix)\n",
    "\n",
    "            if self.shuffle:\n",
    "                X = _shuffle(X)\n",
    "\n",
    "            bu, bi, pu, qi = _run_epoch(\n",
    "                X, bu, bi, pu, qi, self.global_mean_, self.n_factors, self.lr, self.reg\n",
    "            )\n",
    "\n",
    "            if X_val is not None:\n",
    "                self.metrics_.loc[epoch_ix, :] = _compute_val_metrics(\n",
    "                    X_val, bu, bi, pu, qi, self.global_mean_, self.n_factors\n",
    "                )\n",
    "                self._on_epoch_end(\n",
    "                    start,\n",
    "                    self.metrics_.loc[epoch_ix, \"Loss\"],\n",
    "                    self.metrics_.loc[epoch_ix, \"RMSE\"],\n",
    "                    self.metrics_.loc[epoch_ix, \"MAE\"],\n",
    "                )\n",
    "\n",
    "                if self.early_stopping:\n",
    "                    val_rmse = self.metrics_[\"RMSE\"].tolist()\n",
    "                    if self._early_stopping(val_rmse, epoch_ix, self.min_delta):\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                self._on_epoch_end(start)\n",
    "\n",
    "        self.bu_ = bu\n",
    "        self.bi_ = bi\n",
    "        self.pu_ = pu\n",
    "        self.qi_ = qi\n",
    "\n",
    "    def predict(self, X, clip=True):\n",
    "        return [\n",
    "            self.predict_pair(u_id, i_id, clip)\n",
    "            for u_id, i_id in zip(X[\"u_id\"], X[\"i_id\"])\n",
    "        ]\n",
    "\n",
    "    def predict_pair(self, u_id, i_id, clip=True):\n",
    "        user_known, item_known = False, False\n",
    "        pred = self.global_mean_\n",
    "\n",
    "        if u_id in self.user_mapping_:\n",
    "            user_known = True\n",
    "            u_ix = self.user_mapping_[u_id]\n",
    "            pred += self.bu_[u_ix]\n",
    "\n",
    "        if i_id in self.item_mapping_:\n",
    "            item_known = True\n",
    "            i_ix = self.item_mapping_[i_id]\n",
    "            pred += self.bi_[i_ix]\n",
    "\n",
    "        if user_known and item_known:\n",
    "            pred += np.dot(self.pu_[u_ix], self.qi_[i_ix])\n",
    "\n",
    "        if clip:\n",
    "            pred = self.max_rating if pred > self.max_rating else pred\n",
    "            pred = self.min_rating if pred < self.min_rating else pred\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def _early_stopping(self, val_rmse, epoch_idx, min_delta):\n",
    "        if epoch_idx > 0:\n",
    "            if val_rmse[epoch_idx] + min_delta > val_rmse[epoch_idx - 1]:\n",
    "                self.metrics_ = self.metrics_.loc[: (epoch_idx + 1), :]\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _on_epoch_begin(self, epoch_ix):\n",
    "        start = time.time()\n",
    "        end = \"  | \" if epoch_ix < 9 else \" | \"\n",
    "        print(\"Epoch {}/{}\".format(epoch_ix + 1, self.n_epochs), end=end)\n",
    "\n",
    "        return start\n",
    "\n",
    "    def _on_epoch_end(self, start, val_loss=None, val_rmse=None, val_mae=None):\n",
    "        end = time.time()\n",
    "\n",
    "        if val_loss is not None:\n",
    "            print(f\"val_loss: {val_loss:.2f}\", end=\" - \")\n",
    "            print(f\"val_rmse: {val_rmse:.2f}\", end=\" - \")\n",
    "            print(f\"val_mae: {val_mae:.2f}\", end=\" - \")\n",
    "\n",
    "        print(f\"took {end - start:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "\n",
      "Preprocessing data...\n",
      "\n",
      "Epoch 1/100  | val_loss: 1.28 - val_rmse: 1.13 - val_mae: 0.95 - took 3.4 sec\n",
      "Epoch 2/100  | val_loss: 1.28 - val_rmse: 1.13 - val_mae: 0.95 - took 3.2 sec\n",
      "Test MAE: 0.95\n"
     ]
    }
   ],
   "source": [
    "svd = SVD(\n",
    "    lr=0.001,\n",
    "    reg=0.005,\n",
    "    n_epochs=100,\n",
    "    n_factors=15,\n",
    "    early_stopping=True,\n",
    "    shuffle=False,\n",
    "    min_rating=1,\n",
    "    max_rating=5,\n",
    ")\n",
    "svd.fit(X=train, X_val=val)\n",
    "\n",
    "pred = svd.predict(test)\n",
    "mae = mean_absolute_error(test[\"rating\"], pred)\n",
    "\n",
    "print(f\"Test MAE: {mae:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
