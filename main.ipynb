{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import time\n",
    "from functools import wraps\n",
    "from math import trunc\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIANTS object defines data configurations for different file sizes.\n",
    "# Each key represents the data size (e.g., \"100k\" for 100 thousand).\n",
    "VARIANTS = {\n",
    "    \"100k\": {\"filename\": \"u.data\", \"sep\": \"\\t\"},\n",
    "    \"1m\": {\"filename\": \"ratings.dat\", \"sep\": r\"::\"},\n",
    "    \"20m\": {\"filename\": \"ratings.csv\", \"sep\": \",\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chosen data variant (e.g., \"100k\", \"1m\", or \"20m\")\n",
    "variant = \"100k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the chosen variant is a valid key in the VARIANTS object\n",
    "if variant not in VARIANTS:\n",
    "    # If not valid, raise an error\n",
    "    raise ValueError(\n",
    "        f\"Invalid variant: {variant}. Valid options are {list(VARIANTS.keys())}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the URL for downloading the data based on the chosen variant\n",
    "url = f\"http://files.grouplens.org/datasets/movielens/ml-{variant}.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from the VARIANTS object for the chosen variant\n",
    "variant_info = VARIANTS[variant]\n",
    "\n",
    "# Destructure filename property from the variant information\n",
    "filename = variant_info[\"filename\"]\n",
    "\n",
    "# Construct the directory name based on the variant\n",
    "dirname = f\"ml-{variant}\"\n",
    "\n",
    "# Construct the path to the downloaded zip file\n",
    "zip_path = os.path.join(dirname + \".zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHO REMOVE CELL BEN DUOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_url = \"http://26.26.26.1:10809\"\n",
    "\n",
    "proxy_support = urllib.request.ProxyHandler({\"http\": proxy_url})\n",
    "\n",
    "opener = urllib.request.build_opener(proxy_support)\n",
    "\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data from the URL and save it to the zip_path\n",
    "with urllib.request.urlopen(url) as r, open(zip_path, \"wb\") as f:\n",
    "    # Copy the downloaded data from the response to the file\n",
    "    shutil.copyfileobj(r, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the downloaded zip file\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "    # Extract all files from the zip archive to the current directory\n",
    "    zf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zip file after extraction (optional)\n",
    "os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the path to the CSV file\n",
    "csv_path = os.path.join(dirname, filename)\n",
    "\n",
    "# Define the column names for the data\n",
    "names = [\"u_id\", \"i_id\", \"rating\", \"timestamp\"]\n",
    "\n",
    "# Define data type for each column\n",
    "dtype = {\"u_id\": np.uint32, \"i_id\": np.uint32, \"rating\": np.float64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV data into a Pandas DataFrame\n",
    "df = pd.read_csv(\n",
    "    csv_path,\n",
    "    names=names,\n",
    "    dtype=dtype,\n",
    "    header=0,\n",
    "    sep=VARIANTS[variant][\"sep\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"timestamp\" column from the DataFrame\n",
    "df.drop(\"timestamp\", inplace=True, axis=1)\n",
    "\n",
    "# Sort the DataFrame by the \"u_id\" column (assuming user IDs)\n",
    "df.sort_values(by=\"u_id\", inplace=True)\n",
    "\n",
    "# Reset the index after sorting (optional, keeps row numbers aligned with data)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 80% for training set\n",
    "train = df.sample(frac=0.8, random_state=7)\n",
    "\n",
    "# Sample 50% from remaining for validation\n",
    "val = df.drop(train.index.tolist()).sample(frac=0.5, random_state=8)\n",
    "\n",
    "# Remaining data becomes test set\n",
    "test = df.drop(train.index.tolist()).drop(val.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X):\n",
    "    np.random.shuffle(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def initialization(n_users, n_items, n_factors):\n",
    "    # Initialize user biases with zeros (n_users length)\n",
    "    bu = np.zeros(n_users)\n",
    "\n",
    "    # Initialize item biases with zeros (n_items length)\n",
    "    bi = np.zeros(n_items)\n",
    "\n",
    "    # Random user factors (normal distribution, mean 0, std 0.1)\n",
    "    pu = np.random.normal(0, 0.1, (n_users, n_factors))\n",
    "\n",
    "    # Random item factors (normal distribution, mean 0, std 0.1)\n",
    "    qi = np.random.normal(0, 0.1, (n_items, n_factors))\n",
    "\n",
    "    return bu, bi, pu, qi\n",
    "\n",
    "\n",
    "def run_epoch(X, bu, bi, pu, qi, global_mean, n_factors, lr, reg):\n",
    "    # Loop through each rating in the data matrix X\n",
    "    for i in range(X.shape[0]):\n",
    "        # Extract user ID, item ID, and rating from current row\n",
    "        user, item, rating = int(X[i, 0]), int(X[i, 1]), X[i, 2]\n",
    "\n",
    "        # Predict current rating\n",
    "        pred = global_mean + bu[user] + bi[item]\n",
    "        for factor in range(n_factors):\n",
    "            pred += pu[user, factor] * qi[item, factor]\n",
    "\n",
    "        # Calculate the error between predicted and actual rating\n",
    "        err = rating - pred\n",
    "\n",
    "        # Update user and item biases with learning rate (lr) and regularization (reg)\n",
    "        bu[user] += lr * (err - reg * bu[user])\n",
    "        bi[item] += lr * (err - reg * bi[item])\n",
    "\n",
    "        # Update user and item latent factors for all factors (n_factors)\n",
    "        for factor in range(n_factors):\n",
    "            puf = pu[user, factor]\n",
    "            qif = qi[item, factor]\n",
    "\n",
    "            pu[user, factor] += lr * (err * qif - reg * puf)\n",
    "            qi[item, factor] += lr * (err * puf - reg * qif)\n",
    "\n",
    "    return bu, bi, pu, qi\n",
    "\n",
    "\n",
    "def compute_val_metrics(X_val, bu, bi, pu, qi, global_mean, n_factors):\n",
    "    residuals = []\n",
    "\n",
    "    for i in range(X_val.shape[0]):\n",
    "        user, item, rating = int(X_val[i, 0]), int(X_val[i, 1]), X_val[i, 2]\n",
    "        pred = global_mean\n",
    "\n",
    "        if user > -1:\n",
    "            pred += bu[user]\n",
    "\n",
    "        if item > -1:\n",
    "            pred += bi[item]\n",
    "\n",
    "        if (user > -1) and (item > -1):\n",
    "            for factor in range(n_factors):\n",
    "                pred += pu[user, factor] * qi[item, factor]\n",
    "\n",
    "        residuals.append(rating - pred)\n",
    "\n",
    "    residuals = np.array(residuals)\n",
    "    loss = np.square(residuals).mean()\n",
    "    rmse = np.sqrt(loss)\n",
    "    mae = np.absolute(residuals).mean()\n",
    "\n",
    "    return loss, rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = \"\\033[95m\"\n",
    "    OKBLUE = \"\\033[94m\"\n",
    "    OKCYAN = \"\\033[96m\"\n",
    "    OKGREEN = \"\\033[92m\"\n",
    "    WARNING = \"\\033[93m\"\n",
    "    FAIL = \"\\033[91m\"\n",
    "    ENDC = \"\\033[0m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNDERLINE = \"\\033[4m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=0.005,\n",
    "        reg=0.02,\n",
    "        n_epochs=20,\n",
    "        n_factors=100,\n",
    "        early_stopping=True,\n",
    "        shuffle=True,\n",
    "        min_delta=0.001,\n",
    "        min_rating=1,\n",
    "        max_rating=5,\n",
    "    ):\n",
    "\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_factors = n_factors\n",
    "        self.early_stopping = early_stopping\n",
    "        self.shuffle = shuffle\n",
    "        self.min_delta = min_delta\n",
    "        self.min_rating = min_rating\n",
    "        self.max_rating = max_rating\n",
    "\n",
    "    def fit(self, X, X_val=None):\n",
    "        # Preprocess the training data\n",
    "        X = self.preprocess_data(X)\n",
    "\n",
    "        # Check if validation data is provided\n",
    "        if X_val is not None:\n",
    "            # Preprocess the validation data\n",
    "            X_val = self.preprocess_data(X_val, train=False)\n",
    "            # Initialize metrics for evaluation\n",
    "            self.init_metrics()\n",
    "\n",
    "        # Calculate the global mean of the rating feature\n",
    "        self.global_mean_ = np.mean(X[:, 2])\n",
    "\n",
    "        # Run the Stochastic Gradient Descent (SGD) algorithm\n",
    "        self.run_sgd(X, X_val)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def preprocess_data(self, X, train=True):\n",
    "        # Copy the data to avoid modifying the original DataFrame\n",
    "        X = X.copy()\n",
    "\n",
    "        # Mappings are only needed during training\n",
    "        if train:\n",
    "            # Create unique mappings for user and item IDs (if training)\n",
    "            user_ids = X[\"u_id\"].unique().tolist()\n",
    "            item_ids = X[\"i_id\"].unique().tolist()\n",
    "\n",
    "            n_users = len(user_ids)\n",
    "            n_items = len(item_ids)\n",
    "\n",
    "            user_idx = range(n_users)\n",
    "            item_idx = range(n_items)\n",
    "\n",
    "            self.user_mapping_ = dict(zip(user_ids, user_idx))\n",
    "            self.item_mapping_ = dict(zip(item_ids, item_idx))\n",
    "\n",
    "        # Replace user and item IDs with their corresponding indices in the mappings\n",
    "        X[\"u_id\"] = X[\"u_id\"].map(self.user_mapping_)\n",
    "        X[\"i_id\"] = X[\"i_id\"].map(self.item_mapping_)\n",
    "\n",
    "        # Tag unseen users/items in validation data with -1 (for handling unknown entries)\n",
    "        X.fillna(-1, inplace=True)\n",
    "\n",
    "        X[\"u_id\"] = X[\"u_id\"].astype(np.int32)\n",
    "        X[\"i_id\"] = X[\"i_id\"].astype(np.int32)\n",
    "\n",
    "        return X[[\"u_id\", \"i_id\", \"rating\"]].values\n",
    "\n",
    "    def init_metrics(self):\n",
    "        # Initialize empty metrics array with zeros\n",
    "        metrics = np.zeros((self.n_epochs, 3), dtype=float)\n",
    "\n",
    "        # Create a pandas DataFrame from the metrics array\n",
    "        self.metrics_ = pd.DataFrame(metrics, columns=[\"Loss\", \"RMSE\", \"MAE\"])\n",
    "\n",
    "    def run_sgd(self, X, X_val):\n",
    "        # Get number of unique users\n",
    "        n_users = len(np.unique(X[:, 0]))\n",
    "\n",
    "        # Get number of unique items\n",
    "        n_items = len(np.unique(X[:, 1]))\n",
    "\n",
    "        # Initialize model parameters\n",
    "        bu, bi, pu, qi = initialization(n_users, n_items, self.n_factors)\n",
    "\n",
    "        # Run SGD for specified number of epochs\n",
    "        for epoch_ix in range(self.n_epochs):\n",
    "            start = self.on_epoch_begin(epoch_ix)\n",
    "\n",
    "            if self.shuffle:\n",
    "                X = shuffle(X)\n",
    "\n",
    "            # Update model parameters using run_epoch function\n",
    "            bu, bi, pu, qi = run_epoch(\n",
    "                X, bu, bi, pu, qi, self.global_mean_, self.n_factors, self.lr, self.reg\n",
    "            )\n",
    "\n",
    "            if X_val is not None:\n",
    "                # Compute validation metrics if validation data provided\n",
    "                self.metrics_.loc[epoch_ix, :] = compute_val_metrics(\n",
    "                    X_val, bu, bi, pu, qi, self.global_mean_, self.n_factors\n",
    "                )\n",
    "                self.on_epoch_end(\n",
    "                    start,\n",
    "                    self.metrics_.loc[epoch_ix, \"Loss\"],\n",
    "                    self.metrics_.loc[epoch_ix, \"RMSE\"],\n",
    "                    self.metrics_.loc[epoch_ix, \"MAE\"],\n",
    "                )\n",
    "\n",
    "                if self.early_stopping:\n",
    "                    val_rmse = self.metrics_[\"RMSE\"].tolist()\n",
    "                    if self.check_early_stopping(val_rmse, epoch_ix, self.min_delta):\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                self.on_epoch_end(start)\n",
    "\n",
    "        # Update internal model parameters with learned values\n",
    "        self.bu_ = bu\n",
    "        self.bi_ = bi\n",
    "        self.pu_ = pu\n",
    "        self.qi_ = qi\n",
    "\n",
    "    def predict(self, X, clip=True):\n",
    "        # Generate predictions for each user-item pair in X\n",
    "        return [\n",
    "            self.predict_pair(u_id, i_id, clip)\n",
    "            for u_id, i_id in zip(X[\"u_id\"], X[\"i_id\"])\n",
    "        ]\n",
    "\n",
    "    def predict_pair(self, u_id, i_id, clip=True):\n",
    "        # Initialize flags indicating if user and item are known\n",
    "        user_known, item_known = False, False\n",
    "\n",
    "        # Start prediction with global mean rating\n",
    "        pred = self.global_mean_\n",
    "\n",
    "        # Check if user ID exists in user mapping\n",
    "        if u_id in self.user_mapping_:\n",
    "            user_known = True\n",
    "            # Get user index\n",
    "            u_ix = self.user_mapping_[u_id]\n",
    "            # Add user bias\n",
    "            pred += self.bu_[u_ix]\n",
    "\n",
    "        # Check if item ID exists in item mapping\n",
    "        if i_id in self.item_mapping_:\n",
    "            item_known = True\n",
    "            # Get user index\n",
    "            i_ix = self.item_mapping_[i_id]\n",
    "            # Add user bias\n",
    "            pred += self.bi_[i_ix]\n",
    "\n",
    "        # If both user and item are known, add user-item interaction component\n",
    "        if user_known and item_known:\n",
    "            pred += np.dot(self.pu_[u_ix], self.qi_[i_ix])\n",
    "\n",
    "        # Clip the predicted rating to the defined range (if clip is True)\n",
    "        if clip:\n",
    "            pred = self.max_rating if pred > self.max_rating else pred\n",
    "            pred = self.min_rating if pred < self.min_rating else pred\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def check_early_stopping(self, val_rmse, epoch_idx, min_delta):\n",
    "        if epoch_idx > 0:\n",
    "            # Check if validation RMSE has worsened by more than min_delta\n",
    "            if val_rmse[epoch_idx] + min_delta > val_rmse[epoch_idx - 1]:\n",
    "                # Update metrics DataFrame up to the current epoch\n",
    "                self.metrics_ = self.metrics_.loc[: (epoch_idx + 1), :]\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def on_epoch_begin(self, epoch_ix):\n",
    "        start = time.time()\n",
    "        end = \"  | \" if epoch_ix < 9 else \" | \"\n",
    "        print(\n",
    "            f\"{bcolors.WARNING}[EPOCH]:{bcolors.ENDC} {epoch_ix + 1}/{self.n_epochs}\",\n",
    "            end=end,\n",
    "        )\n",
    "\n",
    "        return start\n",
    "\n",
    "    def on_epoch_end(self, start, val_loss=None, val_rmse=None, val_mae=None):\n",
    "        end = time.time()\n",
    "\n",
    "        if val_loss is not None:\n",
    "            print(f\"[VAL_LOSS]: {val_loss:.2f}\", end=\" | \")\n",
    "            print(f\"[VAL_RMSE]: {val_rmse:.2f}\", end=\" | \")\n",
    "            print(f\"[VAL_MAE]: {val_mae:.2f}\", end=\" | \")\n",
    "\n",
    "        print(f\"{bcolors.OKGREEN}[TIME]:{bcolors.ENDC} {end - start:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m[EPOCH]:\u001b[0m 1/100  | [VAL_LOSS]: 1.17 | [VAL_RMSE]: 1.08 | [VAL_MAE]: 0.91 | \u001b[92m[TIME]:\u001b[0m 3.6\n",
      "\u001b[93m[EPOCH]:\u001b[0m 2/100  | [VAL_LOSS]: 1.11 | [VAL_RMSE]: 1.05 | [VAL_MAE]: 0.87 | \u001b[92m[TIME]:\u001b[0m 3.5\n",
      "\u001b[93m[EPOCH]:\u001b[0m 3/100  | [VAL_LOSS]: 1.06 | [VAL_RMSE]: 1.03 | [VAL_MAE]: 0.85 | \u001b[92m[TIME]:\u001b[0m 2.8\n",
      "\u001b[93m[EPOCH]:\u001b[0m 4/100  | [VAL_LOSS]: 1.03 | [VAL_RMSE]: 1.02 | [VAL_MAE]: 0.83 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 5/100  | [VAL_LOSS]: 1.01 | [VAL_RMSE]: 1.00 | [VAL_MAE]: 0.81 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 6/100  | [VAL_LOSS]: 0.99 | [VAL_RMSE]: 1.00 | [VAL_MAE]: 0.80 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 7/100  | [VAL_LOSS]: 0.98 | [VAL_RMSE]: 0.99 | [VAL_MAE]: 0.80 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 8/100  | [VAL_LOSS]: 0.97 | [VAL_RMSE]: 0.98 | [VAL_MAE]: 0.79 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 9/100  | [VAL_LOSS]: 0.96 | [VAL_RMSE]: 0.98 | [VAL_MAE]: 0.78 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 10/100 | [VAL_LOSS]: 0.95 | [VAL_RMSE]: 0.98 | [VAL_MAE]: 0.78 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 11/100 | [VAL_LOSS]: 0.94 | [VAL_RMSE]: 0.97 | [VAL_MAE]: 0.78 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 12/100 | [VAL_LOSS]: 0.94 | [VAL_RMSE]: 0.97 | [VAL_MAE]: 0.77 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 13/100 | [VAL_LOSS]: 0.93 | [VAL_RMSE]: 0.97 | [VAL_MAE]: 0.77 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 14/100 | [VAL_LOSS]: 0.93 | [VAL_RMSE]: 0.96 | [VAL_MAE]: 0.77 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 15/100 | [VAL_LOSS]: 0.93 | [VAL_RMSE]: 0.96 | [VAL_MAE]: 0.77 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 16/100 | [VAL_LOSS]: 0.92 | [VAL_RMSE]: 0.96 | [VAL_MAE]: 0.77 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 17/100 | [VAL_LOSS]: 0.92 | [VAL_RMSE]: 0.96 | [VAL_MAE]: 0.76 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 18/100 | [VAL_LOSS]: 0.92 | [VAL_RMSE]: 0.96 | [VAL_MAE]: 0.76 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 19/100 | [VAL_LOSS]: 0.92 | [VAL_RMSE]: 0.96 | [VAL_MAE]: 0.76 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 20/100 | [VAL_LOSS]: 0.91 | [VAL_RMSE]: 0.96 | [VAL_MAE]: 0.76 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 21/100 | [VAL_LOSS]: 0.91 | [VAL_RMSE]: 0.95 | [VAL_MAE]: 0.76 | \u001b[92m[TIME]:\u001b[0m 2.1\n",
      "\u001b[93m[EPOCH]:\u001b[0m 22/100 | [VAL_LOSS]: 0.91 | [VAL_RMSE]: 0.95 | [VAL_MAE]: 0.76 | \u001b[92m[TIME]:\u001b[0m 2.1\n"
     ]
    }
   ],
   "source": [
    "svd = SVD(\n",
    "    lr=0.001,\n",
    "    reg=0.005,\n",
    "    n_epochs=100,\n",
    "    n_factors=15,\n",
    "    early_stopping=True,\n",
    "    shuffle=True,\n",
    "    min_rating=1,\n",
    "    max_rating=5,\n",
    ")\n",
    "svd.fit(X=train, X_val=val)\n",
    "\n",
    "pred = svd.predict(test)\n",
    "rmse = root_mean_squared_error(test[\"rating\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.96\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test RMSE: {rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
